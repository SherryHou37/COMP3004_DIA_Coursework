{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e167c0c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from models.miniTransformer import generate_square_subsequent_mask\n",
    "from models.miniTransformerV2 import TransformerChat\n",
    "import myTokenizer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7b11f3c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Tokenizer is loaded successfully: /tokenizer/tokenizerForHealthCare.pkl\n",
      "tokenizer done, with length 41978\n",
      "vocab size: 15000\n"
     ]
    }
   ],
   "source": [
    "from myTokenizer import myTokenizer\n",
    "ThisTokenizer = myTokenizer(num_words=15000)\n",
    "\n",
    "TOKENIZER = ThisTokenizer.load_tokenizer('/tokenizer/tokenizerForHealthCare.pkl')\n",
    "print(\"tokenizer done, with length\", len(TOKENIZER.word_index) + 1)\n",
    "print(\"vocab size:\", TOKENIZER.num_words)\n",
    "\n",
    "# word → index\n",
    "word2idx = TOKENIZER.word_index\n",
    "\n",
    "# index → word\n",
    "idx2word = {idx: word for word, idx in word2idx.items()}\n",
    "\n",
    "# 注意：word_index 不会自动添加 <pad>，如果你在训练时加了 pad_idx=0，要手动加：\n",
    "word2idx[\"<pad>\"] = 0\n",
    "idx2word[0] = \"<pad>\"\n",
    "\n",
    "# src_vocab 和 trg_vocab 就是这个 word2idx（如果是共享词表的话）\n",
    "src_vocab = word2idx\n",
    "trg_vocab = word2idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "008114b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token ID for '<start>': 18\n",
      "Token ID for '<end>': 19\n"
     ]
    }
   ],
   "source": [
    "SOS_IDX = TOKENIZER.word_index.get(\"<start>\", \"<Not found>\")\n",
    "print(\"Token ID for '<start>':\", SOS_IDX)\n",
    "\n",
    "EOS_IDX = TOKENIZER.word_index.get(\"<end>\", \"<Not found>\")\n",
    "print(\"Token ID for '<end>':\", EOS_IDX)\n",
    "\n",
    "PAD_IDX = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bcb68a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.BiGRU import EncoderBiGRU, DecoderGRU, Seq2SeqGRU, BahdanauAttention\n",
    "\n",
    "# 假设你已经知道下面这些参数：\n",
    "vocab_size = TOKENIZER.num_words + 1  \n",
    "embedding_dim = 256\n",
    "hidden_dim = 512\n",
    "pad_idx = 0\n",
    "output_dim = vocab_size  # 生成任务，输出词表大小和输入相同\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "encoder = EncoderBiGRU(vocab_size, embedding_dim, hidden_dim, pad_idx)\n",
    "attention = BahdanauAttention(hidden_dim)\n",
    "decoder = DecoderGRU(vocab_size, embedding_dim, hidden_dim, output_dim, pad_idx, attention)\n",
    "\n",
    "# Initialize the Seq2Seq model\n",
    "model = Seq2SeqGRU(encoder, decoder, pad_idx, DEVICE).to(DEVICE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0cacdcea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Seq2SeqGRU(\n",
       "  (encoder): EncoderBiGRU(\n",
       "    (embedding): Embedding(15001, 256, padding_idx=0)\n",
       "    (gru): GRU(256, 512, batch_first=True, bidirectional=True)\n",
       "  )\n",
       "  (decoder): DecoderGRU(\n",
       "    (embedding): Embedding(15001, 256, padding_idx=0)\n",
       "    (gru): GRU(1280, 512, batch_first=True)\n",
       "    (fc_out): Linear(in_features=512, out_features=15001, bias=True)\n",
       "    (attention): BahdanauAttention(\n",
       "      (attn): Linear(in_features=1536, out_features=512, bias=True)\n",
       "      (v): Linear(in_features=512, out_features=1, bias=False)\n",
       "    )\n",
       "  )\n",
       "  (bridge): Linear(in_features=1024, out_features=512, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 加载保存的参数\n",
    "model.load_state_dict(torch.load(\"checkpoint/weight_biGRU_1550.pth\", map_location=DEVICE))\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7a310a9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from typing import List\n",
    "\n",
    "@torch.inference_mode()      # PyTorch ≥1.12，自动关掉梯度\n",
    "def health_chat(\n",
    "    sentence: str,\n",
    "    model: \"Seq2SeqGRU\",\n",
    "    tokenizer_src,\n",
    "    tokenizer_trg,\n",
    "    device: torch.device,\n",
    "    max_len: int = 100,\n",
    "    sos_idx: int = SOS_IDX,\n",
    "    eos_idx: int = EOS_IDX,\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    用训练好的 Seq2SeqGRU 进行推理，返回译文字符串\n",
    "    --------------------------------------------------\n",
    "    sentence     : 原始输入句子（str）\n",
    "    tokenizer_*  : 训练时用的分词器；如果是同一个词表就都传 tokenizer_src\n",
    "    model        : 加载好权重并切到 model.eval() 的 Seq2SeqGRU\n",
    "    device       : torch.device(\"cuda\") / torch.device(\"cpu\")\n",
    "    max_len      : 生成时的最长长度上限\n",
    "    sos_idx      : <sos> 的 id\n",
    "    eos_idx      : <eos> 的 id\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    # 1. 预处理 —— 分词 → id → tensor\n",
    "    src_ids = tokenizer_src.texts_to_sequences([sentence])[0] \n",
    "    \n",
    "    src_ids = [sos_idx] + src_ids + [eos_idx]\n",
    "\n",
    "    src_tensor = torch.tensor(src_ids, dtype=torch.long, device=device).unsqueeze(0)  # [1, src_len]\n",
    "\n",
    "    # 2. Encoder\n",
    "    encoder_outs, enc_hidden = model.encoder(src_tensor)             # [1, src_len, 2H], [1, 2H]  :contentReference[oaicite:0]{index=0}:contentReference[oaicite:1]{index=1}\n",
    "    dec_hidden = torch.tanh(model.bridge(enc_hidden))                # [1, H]          :contentReference[oaicite:2]{index=2}:contentReference[oaicite:3]{index=3}\n",
    "    dec_hidden = dec_hidden.unsqueeze(0)                             # [1, 1, H]      :contentReference[oaicite:4]{index=4}:contentReference[oaicite:5]{index=5}\n",
    "\n",
    "    # 3. Decoder – 逐步生成，greedy search\n",
    "    trg_indices: List[int] = [sos_idx]                               # 先放 <sos>\n",
    "    for _ in range(max_len):\n",
    "        # 上一步输出（或 <sos>）作为当前输入\n",
    "        last_token = torch.tensor([trg_indices[-1]], device=device)  # [1]\n",
    "        output, dec_hidden, _ = model.decoder(\n",
    "            last_token, dec_hidden, encoder_outs\n",
    "        )                                                            # output: [1, vocab]  :contentReference[oaicite:6]{index=6}:contentReference[oaicite:7]{index=7}\n",
    "\n",
    "        next_token = int(output.argmax(1))                           # greedy\n",
    "        trg_indices.append(next_token)\n",
    "\n",
    "        if next_token == eos_idx:\n",
    "            break\n",
    "\n",
    "    # 4. 去掉首尾标记，转回文本\n",
    "    trg_tokens = [tokenizer_trg.index_word[i] for i in trg_indices[1:-1]]  # id → token\n",
    "    translation = \" \".join(trg_tokens)  \n",
    "    return translation.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ea0381d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "翻译结果： hi welcome to chat doctor . i read and understood your problem . pain lower back pain is suggestive of musculoskeletal pain or some skin , increasing pain increasing some neck pain increasing musculoskeletal pain increasing increasing some bones are some any further assessment .\n"
     ]
    }
   ],
   "source": [
    "sentence = \"i feel like i have persistent knee pain that has been going on for two days.\"\n",
    "src_tokens = [src_vocab.get(tok, src_vocab['<UNKNOWN>']) for tok in sentence.split()]\n",
    "src_tensor = torch.LongTensor(src_tokens)\n",
    "\n",
    "output = health_chat(sentence, model, TOKENIZER,TOKENIZER, DEVICE)\n",
    "print(\"翻译结果：\", output)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dia",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
