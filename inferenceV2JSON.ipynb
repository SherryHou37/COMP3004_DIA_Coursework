{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e167c0c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from models.miniTransformer import generate_square_subsequent_mask\n",
    "from models.miniTransformerV2 import TransformerChat\n",
    "import myTokenizer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7a310a9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def decode_with_topk_penalty(model, tokenizer, input_text, k=10, max_len=100, temperature=1.2, penalty=1.5, device='cpu'):\n",
    "    model.eval()\n",
    "    start_id = tokenizer.word_index['<start>']\n",
    "    end_id = tokenizer.word_index['<end>']\n",
    "\n",
    "    cleaned = myTokenizer.clean_text(input_text)\n",
    "    input_ids = tokenizer.texts_to_sequences([cleaned])\n",
    "    input_tensor = torch.tensor(input_ids).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        src_emb = model.pos_encoder(model.embedding(input_tensor))\n",
    "        memory = model.transformer.encoder(model.norm(src_emb))\n",
    "\n",
    "    decoder_input = torch.tensor([[start_id]], device=device)\n",
    "\n",
    "    for _ in range(max_len):\n",
    "        tgt_emb = model.pos_encoder(model.embedding(decoder_input))\n",
    "        tgt_mask = generate_square_subsequent_mask(decoder_input.size(1)).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output = model.transformer.decoder(tgt_emb, memory, tgt_mask=tgt_mask)\n",
    "            logits = model.fc_out(model.norm(output[:, -1, :])) / temperature\n",
    "\n",
    "        # Repetition penalty\n",
    "        for tok in set(decoder_input[0].tolist()):\n",
    "            logits[0, tok] -= penalty\n",
    "\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        topk_probs, topk_ids = torch.topk(probs, k)\n",
    "        next_token = topk_ids[0, torch.multinomial(topk_probs[0], 1)]\n",
    "\n",
    "        decoder_input = torch.cat([decoder_input, next_token.view(1, 1)], dim=1)\n",
    "        if next_token.item() == end_id:\n",
    "            break\n",
    "\n",
    "    output_tokens = decoder_input.squeeze().tolist()[1:]\n",
    "    return ' '.join([tokenizer.index_word.get(i, '<UNK>') for i in output_tokens if i != end_id])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ea0381d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Tokenizer is loaded successfully: /tokenizer/tokenizerForMentalHealth.pkl\n",
      "ü§ñ Bot: i am down up back from other necessary down <UNKNOWN> terrible nightmare or ? as that i reading anything anything vet back back back her her her up back mindfulness meditation diagnosis differently until next next because because because changed violent back community seriously is an better down them or down terrible them . next next next necessary others fully fully down down dangerous them were next past worse were again wanted into low next next she she felt she assumed he she or down them or or anyone someone they were so it because the next ? next next\n",
      "ü§ñ Bot: thanks up those those too those coping tools <UNKNOWN> present or those patterns <UNKNOWN> , steps eight ansiedad story brief insurance nightmare story ? other dreams nightmare whatever action employer de y down breathing j self-esteem pets from deep boundary them him him abusive past past their present them those , . . present those them the present those those those present side marriage present , , <UNKNOWN> <UNKNOWN> again next too too marriage give another marriage him her too marriage doing you--if next down them it; next too loved them them get down them them next next story ?\n"
     ]
    }
   ],
   "source": [
    "from myTokenizer import myTokenizer  \n",
    "# Âä†ËΩΩ tokenizer & Ê®°Âûã\n",
    "ThisTokenizer = myTokenizer(num_words=10000)\n",
    "tokenizer = ThisTokenizer.load_tokenizer('/tokenizer/tokenizerForMentalHealth.pkl')  # ‰øÆÊîπ‰∏∫ÂÆûÈôÖË∑ØÂæÑ\n",
    "vocab_size = tokenizer.num_words + 1\n",
    "\n",
    "model = TransformerChat(vocab_size=vocab_size)\n",
    "model.load_state_dict(torch.load('checkpoint/weight_transformerV2_JSON_1550.pth', map_location='cuda'))  # ‰øÆÊîπ‰∏∫‰Ω†‰øùÂ≠òÁöÑÊ®°ÂûãË∑ØÂæÑ\n",
    "model = model.to('cuda')  # Êàñ 'cuda'\n",
    "\n",
    "response = decode_with_topk_penalty(model, tokenizer, \"I feel tired and dizzy\", device='cuda')\n",
    "print(\"ü§ñ Bot:\", response)\n",
    "\n",
    "test2 = \"\"\"My doctor had issue with finding the baby during my transvaginal ultrasound is that because I had to pee she could see the yoke sac and I measured 9 weeks but what she thought was the baby was measuring 7 weeks. I am just worried and want to know if that interfered\"\"\"\n",
    "\n",
    "response = decode_with_topk_penalty(model, tokenizer, test2, device='cuda')\n",
    "print(\"ü§ñ Bot:\", response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c6a5fb34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 14691\n",
      "Number of words: 13001\n",
      "<UNKNOWN>: 1\n",
      ".: 2\n",
      "to: 3\n",
      ",: 4\n",
      "you: 5\n",
      "and: 6\n",
      "i: 7\n",
      "the: 8\n",
      "a: 9\n",
      "is: 10\n",
      "that: 11\n",
      "of: 12\n",
      "your: 13\n",
      "in: 14\n",
      "it: 15\n",
      "are: 16\n",
      "Token ID for '<start>': 22\n"
     ]
    }
   ],
   "source": [
    "word2idx = tokenizer.word_index\n",
    "\n",
    "# show the length of the vocabulary\n",
    "print(f\"Vocabulary size: {len(word2idx)}\")\n",
    "print(f\"Number of words: {tokenizer.num_words + 1}\")\n",
    "\n",
    "idx2word = {v: k for k, v in word2idx.items()}\n",
    "\n",
    "for word, idx in word2idx.items():\n",
    "    print(f\"{word}: {idx}\")\n",
    "    if idx > 15:\n",
    "        break\n",
    "\n",
    "token_id = tokenizer.word_index.get(\"<start>\", \"<Not found>\")\n",
    "print(\"Token ID for '<start>':\", token_id)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dia",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
