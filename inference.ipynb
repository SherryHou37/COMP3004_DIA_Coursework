{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e167c0c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from models.miniTransformer import generate_square_subsequent_mask, TransformerChat\n",
    "import myTokenizer\n",
    "\n",
    "def greedy_decode(model, tokenizer, input_text, max_output_len=150, device='cpu'):\n",
    "    model.eval()\n",
    "\n",
    "    # Step 1: 清洗并编码输入文本\n",
    "    cleaned = myTokenizer.clean_text(input_text)\n",
    "    print(f\"cleaned: {cleaned}\")\n",
    "    input_seq = tokenizer.texts_to_sequences([cleaned])\n",
    "    print(f\"input_seq: {input_seq}\")\n",
    "    input_tensor = torch.tensor(input_seq).to(device)\n",
    "\n",
    "    # Step 2: 准备 decoder 输入（以 <start> 开头）\n",
    "    start_token_id = tokenizer.word_index.get('<start>', 1)\n",
    "    print(f\"start_token_id: {start_token_id}\")\n",
    "    end_token_id = tokenizer.word_index.get('<end>', 2)\n",
    "    decoder_input = torch.tensor([[start_token_id]], device=device)\n",
    "\n",
    "    # Step 3: 编码器输出\n",
    "    with torch.no_grad():\n",
    "        src_emb = model.pos_encoder(model.src_embedding(input_tensor))\n",
    "        memory = model.transformer.encoder(src_emb)\n",
    "\n",
    "    # Step 4: 逐步生成 token\n",
    "    for _ in range(max_output_len):\n",
    "        tgt_emb = model.pos_encoder(model.tgt_embedding(decoder_input))\n",
    "        tgt_mask = generate_square_subsequent_mask(decoder_input.size(1)).to(device)\n",
    "\n",
    "        # with torch.no_grad():\n",
    "        #     output = model.transformer.decoder(\n",
    "        #         tgt_emb, memory, tgt_mask=tgt_mask\n",
    "        #     )\n",
    "        #     logits = model.fc_out(output[:, -1, :])  # 最后一个 token 的输出\n",
    "        #     next_token = logits.argmax(dim=-1).unsqueeze(0)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output = model.transformer.decoder(tgt_emb, memory, tgt_mask=tgt_mask)\n",
    "            logits = model.fc_out(output[:, -1, :])\n",
    "            # print(logits)\n",
    "            # 惩罚重复 <start>（避免死循环）\n",
    "            if decoder_input[0, -1].item() == start_token_id:\n",
    "                logits[0, start_token_id] -= 1.0\n",
    "\n",
    "            next_token = logits.argmax(dim=-1).unsqueeze(0)\n",
    "\n",
    "        decoder_input = torch.cat([decoder_input, next_token], dim=1)\n",
    "\n",
    "        if next_token.item() == end_token_id:\n",
    "            break\n",
    "\n",
    "    # Step 5: 解码输出为文本\n",
    "    output_tokens = decoder_input.squeeze().tolist()[1:]  # 去掉 <start>\n",
    "    words = [tokenizer.index_word.get(tok, '<UNK>') for tok in output_tokens if tok != end_token_id]\n",
    "    return ' '.join(words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fa0c5ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "def beam_search_decode(model, tokenizer, input_text, beam_width=5, max_output_len=100, device='cuda'):\n",
    "    model.eval()\n",
    "\n",
    "    start_token = tokenizer.word_index.get('<start>')\n",
    "    end_token = tokenizer.word_index.get('<end>')\n",
    "    if start_token is None or end_token is None:\n",
    "        raise ValueError(\"Tokenizer must contain <start> and <end> tokens.\")\n",
    "\n",
    "    # Step 1: 预处理输入\n",
    "    cleaned = myTokenizer.clean_text(input_text)\n",
    "    input_seq = tokenizer.texts_to_sequences([cleaned])\n",
    "    input_tensor = torch.tensor(input_seq).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        src_emb = model.pos_encoder(model.src_embedding(input_tensor))\n",
    "        memory = model.transformer.encoder(src_emb)\n",
    "\n",
    "    # 初始beam：[(句子token列表, score)]\n",
    "    beams = [(torch.tensor([[start_token]], device=device), 0.0)]\n",
    "\n",
    "    for _ in range(max_output_len):\n",
    "        new_beams = []\n",
    "        for seq, score in beams:\n",
    "            if seq[0, -1].item() == end_token:\n",
    "                new_beams.append((seq, score))  # 已完成\n",
    "                continue\n",
    "\n",
    "            tgt_emb = model.pos_encoder(model.tgt_embedding(seq))\n",
    "            tgt_mask = generate_square_subsequent_mask(seq.size(1)).to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                output = model.transformer.decoder(tgt_emb, memory, tgt_mask=tgt_mask)\n",
    "                logits = model.fc_out(output[:, -1, :])\n",
    "                log_probs = torch.nn.functional.log_softmax(logits, dim=-1)\n",
    "\n",
    "            topk_log_probs, topk_indices = torch.topk(log_probs, beam_width)\n",
    "\n",
    "            for log_prob, idx in zip(topk_log_probs[0], topk_indices[0]):\n",
    "                next_seq = torch.cat([seq, idx.view(1, 1)], dim=1)\n",
    "                new_beams.append((next_seq, score + log_prob.item()))\n",
    "\n",
    "        # 选出分数最高的前 k 个\n",
    "        beams = sorted(new_beams, key=lambda x: x[1], reverse=True)[:beam_width]\n",
    "\n",
    "        # 如果所有 beam 都结束了，提前终止\n",
    "        if all(seq[0, -1].item() == end_token for seq, _ in beams):\n",
    "            break\n",
    "\n",
    "    # 选得分最高的一条\n",
    "    best_seq = beams[0][0].squeeze().tolist()\n",
    "\n",
    "    # 去掉 <start> 和 <end>\n",
    "    decoded = [\n",
    "        tokenizer.index_word.get(tok, '<UNK>')\n",
    "        for tok in best_seq\n",
    "        if tok not in [start_token, end_token]\n",
    "    ]\n",
    "    return ' '.join(decoded)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7a310a9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "\n",
    "def top_k_sampling_decode(model, tokenizer, input_text, k=10, max_output_len=100, device='cpu'):\n",
    "    model.eval()\n",
    "\n",
    "    start_token = tokenizer.word_index.get('<start>')\n",
    "    end_token = tokenizer.word_index.get('<end>')\n",
    "    # if start_token is None or end_token is None:\n",
    "    #     raise ValueError(\"Tokenizer must contain <start> and <end> tokens.\")\n",
    "\n",
    "    # Step 1: 清洗 + 编码输入\n",
    "    cleaned = myTokenizer.clean_text(input_text)\n",
    "    input_seq = tokenizer.texts_to_sequences([cleaned])\n",
    "    input_tensor = torch.tensor(input_seq).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        src_emb = model.pos_encoder(model.src_embedding(input_tensor))\n",
    "        memory = model.transformer.encoder(src_emb)\n",
    "\n",
    "    # Step 2: 初始化 decoder 输入\n",
    "    decoder_input = torch.tensor([[start_token]], device=device)\n",
    "\n",
    "    for _ in range(max_output_len):\n",
    "        tgt_emb = model.pos_encoder(model.tgt_embedding(decoder_input))\n",
    "        tgt_mask = generate_square_subsequent_mask(decoder_input.size(1)).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output = model.transformer.decoder(tgt_emb, memory, tgt_mask=tgt_mask)\n",
    "            logits = model.fc_out(output[:, -1, :])  # 取最后一个 token 的 logits\n",
    "            # logits = model.fc_out(output[:, -1, :])  # 原始 logits\n",
    "\n",
    "            # 🔁 Repetition penalty\n",
    "            repetition_penalty = 1.5\n",
    "            used_tokens = set(decoder_input[0].tolist())\n",
    "            for token_id in used_tokens:\n",
    "                logits[0, token_id] -= repetition_penalty\n",
    "\n",
    "            # probs = F.softmax(logits, dim=-1)\n",
    "\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "\n",
    "        # Step 3: top-k 采样\n",
    "        topk_probs, topk_indices = torch.topk(probs, k)\n",
    "        topk_probs = topk_probs.squeeze()\n",
    "        topk_indices = topk_indices.squeeze()\n",
    "\n",
    "        print(\"Topk tokens:\", [tokenizer.index_word.get(i.item()) for i in topk_indices])\n",
    "        print(\"Topk probs:\", topk_probs.tolist())\n",
    "\n",
    "        # 从 top-k 中随机选一个\n",
    "        sampled_index = torch.multinomial(topk_probs, 1).item()\n",
    "        next_token = topk_indices[sampled_index]\n",
    "\n",
    "        decoder_input = torch.cat([decoder_input, next_token.view(1, 1)], dim=1)\n",
    "\n",
    "        if next_token.item() == end_token:\n",
    "            break\n",
    "\n",
    "    # 解码为文本\n",
    "    output_tokens = decoder_input.squeeze().tolist()[1:]  # 去掉 <start>\n",
    "    decoded = [\n",
    "        tokenizer.index_word.get(tok, '<UNK>')\n",
    "        for tok in output_tokens if tok != end_token\n",
    "    ]\n",
    "    return ' '.join(decoded)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ea0381d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Tokenizer is loaded successfully: /tokenizer/tokenizerForHealthCare.pkl\n",
      "🤖 Bot: welcome welcome welcome welcome welcome welcome welcome welcome welcome welcome welcome welcome welcome welcome welcome welcome welcome welcome welcome welcome the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n"
     ]
    }
   ],
   "source": [
    "from myTokenizer import myTokenizer  \n",
    "# 加载 tokenizer & 模型\n",
    "ThisTokenizer = myTokenizer(num_words=10000)\n",
    "tokenizer = ThisTokenizer.load_tokenizer('/tokenizer/tokenizerForHealthCare.pkl')  # 修改为实际路径\n",
    "vocab_size = tokenizer.num_words + 1\n",
    "\n",
    "model = TransformerChat(input_vocab_size=vocab_size, target_vocab_size=vocab_size)\n",
    "model.load_state_dict(torch.load('checkpoint/weight_transformer_3550.pth', map_location='cuda'))  # 修改为你保存的模型路径\n",
    "model = model.to('cuda')  # 或 'cuda'\n",
    "\n",
    "# 进行推理\n",
    "# response = greedy_decode(model, tokenizer, \"I feel tired and dizzy\", device='cuda')\n",
    "response = beam_search_decode(model, tokenizer, \"MY HEAD FEEL TERRIBALE\", beam_width=5, device='cuda')\n",
    "# response = top_k_sampling_decode(\n",
    "#     model, tokenizer,\n",
    "#     input_text=\"I feel tired and dizzy\",\n",
    "#     k=10,\n",
    "#     device='cuda'  # 或 'cpu'\n",
    "# )\n",
    "print(\"🤖 Bot:\", response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c6a5fb34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 41977\n",
      "Number of words: 15001\n",
      "<UNKNOWN>: 1\n",
      ".: 2\n",
      ",: 3\n",
      "i: 4\n",
      "and: 5\n",
      "the: 6\n",
      "to: 7\n",
      "a: 8\n",
      "is: 9\n",
      "of: 10\n",
      "you: 11\n",
      "in: 12\n",
      "for: 13\n",
      "it: 14\n",
      "your: 15\n",
      "my: 16\n",
      "Token ID for '<start>': 18\n"
     ]
    }
   ],
   "source": [
    "word2idx = tokenizer.word_index\n",
    "\n",
    "# show the length of the vocabulary\n",
    "print(f\"Vocabulary size: {len(word2idx)}\")\n",
    "print(f\"Number of words: {tokenizer.num_words + 1}\")\n",
    "\n",
    "idx2word = {v: k for k, v in word2idx.items()}\n",
    "\n",
    "for word, idx in word2idx.items():\n",
    "    print(f\"{word}: {idx}\")\n",
    "    if idx > 15:\n",
    "        break\n",
    "\n",
    "token_id = tokenizer.word_index.get(\"<start>\", \"<Not found>\")\n",
    "print(\"Token ID for '<start>':\", token_id)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dia",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
