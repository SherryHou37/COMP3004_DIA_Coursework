{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e167c0c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from models.miniTransformer import generate_square_subsequent_mask, TransformerChat\n",
    "import myTokenizer\n",
    "\n",
    "def greedy_decode(model, tokenizer, input_text, max_output_len=150, device='cpu'):\n",
    "    model.eval()\n",
    "\n",
    "    # Step 1: æ¸…æ´—å¹¶ç¼–ç è¾“å…¥æ–‡æœ¬\n",
    "    cleaned = myTokenizer.clean_text(input_text)\n",
    "    print(f\"cleaned: {cleaned}\")\n",
    "    input_seq = tokenizer.texts_to_sequences([cleaned])\n",
    "    print(f\"input_seq: {input_seq}\")\n",
    "    input_tensor = torch.tensor(input_seq).to(device)\n",
    "\n",
    "    # Step 2: å‡†å¤‡ decoder è¾“å…¥ï¼ˆä»¥ <start> å¼€å¤´ï¼‰\n",
    "    start_token_id = tokenizer.word_index.get('<start>', 1)\n",
    "    print(f\"start_token_id: {start_token_id}\")\n",
    "    end_token_id = tokenizer.word_index.get('<end>', 2)\n",
    "    decoder_input = torch.tensor([[start_token_id]], device=device)\n",
    "\n",
    "    # Step 3: ç¼–ç å™¨è¾“å‡º\n",
    "    with torch.no_grad():\n",
    "        src_emb = model.pos_encoder(model.src_embedding(input_tensor))\n",
    "        memory = model.transformer.encoder(src_emb)\n",
    "\n",
    "    # Step 4: é€æ­¥ç”Ÿæˆ token\n",
    "    for _ in range(max_output_len):\n",
    "        tgt_emb = model.pos_encoder(model.tgt_embedding(decoder_input))\n",
    "        tgt_mask = generate_square_subsequent_mask(decoder_input.size(1)).to(device)\n",
    "\n",
    "        # with torch.no_grad():\n",
    "        #     output = model.transformer.decoder(\n",
    "        #         tgt_emb, memory, tgt_mask=tgt_mask\n",
    "        #     )\n",
    "        #     logits = model.fc_out(output[:, -1, :])  # æœ€åä¸€ä¸ª token çš„è¾“å‡º\n",
    "        #     next_token = logits.argmax(dim=-1).unsqueeze(0)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output = model.transformer.decoder(tgt_emb, memory, tgt_mask=tgt_mask)\n",
    "            logits = model.fc_out(output[:, -1, :])\n",
    "            # print(logits)\n",
    "            # æƒ©ç½šé‡å¤ <start>ï¼ˆé¿å…æ­»å¾ªç¯ï¼‰\n",
    "            if decoder_input[0, -1].item() == start_token_id:\n",
    "                logits[0, start_token_id] -= 1.0\n",
    "\n",
    "            next_token = logits.argmax(dim=-1).unsqueeze(0)\n",
    "\n",
    "        decoder_input = torch.cat([decoder_input, next_token], dim=1)\n",
    "\n",
    "        if next_token.item() == end_token_id:\n",
    "            break\n",
    "\n",
    "    # Step 5: è§£ç è¾“å‡ºä¸ºæ–‡æœ¬\n",
    "    output_tokens = decoder_input.squeeze().tolist()[1:]  # å»æ‰ <start>\n",
    "    words = [tokenizer.index_word.get(tok, '<UNK>') for tok in output_tokens if tok != end_token_id]\n",
    "    return ' '.join(words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fa0c5ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "def beam_search_decode(model, tokenizer, input_text, beam_width=5, max_output_len=100, device='cuda'):\n",
    "    model.eval()\n",
    "\n",
    "    start_token = tokenizer.word_index.get('<start>')\n",
    "    end_token = tokenizer.word_index.get('<end>')\n",
    "    if start_token is None or end_token is None:\n",
    "        raise ValueError(\"Tokenizer must contain <start> and <end> tokens.\")\n",
    "\n",
    "    # Step 1: é¢„å¤„ç†è¾“å…¥\n",
    "    cleaned = myTokenizer.clean_text(input_text)\n",
    "    input_seq = tokenizer.texts_to_sequences([cleaned])\n",
    "    input_tensor = torch.tensor(input_seq).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        src_emb = model.pos_encoder(model.src_embedding(input_tensor))\n",
    "        memory = model.transformer.encoder(src_emb)\n",
    "\n",
    "    # åˆå§‹beamï¼š[(å¥å­tokenåˆ—è¡¨, score)]\n",
    "    beams = [(torch.tensor([[start_token]], device=device), 0.0)]\n",
    "\n",
    "    for _ in range(max_output_len):\n",
    "        new_beams = []\n",
    "        for seq, score in beams:\n",
    "            if seq[0, -1].item() == end_token:\n",
    "                new_beams.append((seq, score))  # å·²å®Œæˆ\n",
    "                continue\n",
    "\n",
    "            tgt_emb = model.pos_encoder(model.tgt_embedding(seq))\n",
    "            tgt_mask = generate_square_subsequent_mask(seq.size(1)).to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                output = model.transformer.decoder(tgt_emb, memory, tgt_mask=tgt_mask)\n",
    "                logits = model.fc_out(output[:, -1, :])\n",
    "                log_probs = torch.nn.functional.log_softmax(logits, dim=-1)\n",
    "\n",
    "            topk_log_probs, topk_indices = torch.topk(log_probs, beam_width)\n",
    "\n",
    "            for log_prob, idx in zip(topk_log_probs[0], topk_indices[0]):\n",
    "                next_seq = torch.cat([seq, idx.view(1, 1)], dim=1)\n",
    "                new_beams.append((next_seq, score + log_prob.item()))\n",
    "\n",
    "        # é€‰å‡ºåˆ†æ•°æœ€é«˜çš„å‰ k ä¸ª\n",
    "        beams = sorted(new_beams, key=lambda x: x[1], reverse=True)[:beam_width]\n",
    "\n",
    "        # å¦‚æœæ‰€æœ‰ beam éƒ½ç»“æŸäº†ï¼Œæå‰ç»ˆæ­¢\n",
    "        if all(seq[0, -1].item() == end_token for seq, _ in beams):\n",
    "            break\n",
    "\n",
    "    # é€‰å¾—åˆ†æœ€é«˜çš„ä¸€æ¡\n",
    "    best_seq = beams[0][0].squeeze().tolist()\n",
    "\n",
    "    # å»æ‰ <start> å’Œ <end>\n",
    "    decoded = [\n",
    "        tokenizer.index_word.get(tok, '<UNK>')\n",
    "        for tok in best_seq\n",
    "        if tok not in [start_token, end_token]\n",
    "    ]\n",
    "    return ' '.join(decoded)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7a310a9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "\n",
    "def top_k_sampling_decode(model, tokenizer, input_text, k=10, max_output_len=100, device='cpu'):\n",
    "    model.eval()\n",
    "\n",
    "    start_token = tokenizer.word_index.get('<start>')\n",
    "    end_token = tokenizer.word_index.get('<end>')\n",
    "    # if start_token is None or end_token is None:\n",
    "    #     raise ValueError(\"Tokenizer must contain <start> and <end> tokens.\")\n",
    "\n",
    "    # Step 1: æ¸…æ´— + ç¼–ç è¾“å…¥\n",
    "    cleaned = myTokenizer.clean_text(input_text)\n",
    "    input_seq = tokenizer.texts_to_sequences([cleaned])\n",
    "    input_tensor = torch.tensor(input_seq).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        src_emb = model.pos_encoder(model.src_embedding(input_tensor))\n",
    "        memory = model.transformer.encoder(src_emb)\n",
    "\n",
    "    # Step 2: åˆå§‹åŒ– decoder è¾“å…¥\n",
    "    decoder_input = torch.tensor([[start_token]], device=device)\n",
    "\n",
    "    for _ in range(max_output_len):\n",
    "        tgt_emb = model.pos_encoder(model.tgt_embedding(decoder_input))\n",
    "        tgt_mask = generate_square_subsequent_mask(decoder_input.size(1)).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output = model.transformer.decoder(tgt_emb, memory, tgt_mask=tgt_mask)\n",
    "            logits = model.fc_out(output[:, -1, :])  # å–æœ€åä¸€ä¸ª token çš„ logits\n",
    "            # logits = model.fc_out(output[:, -1, :])  # åŸå§‹ logits\n",
    "\n",
    "            # ğŸ” Repetition penalty\n",
    "            repetition_penalty = 1.5\n",
    "            used_tokens = set(decoder_input[0].tolist())\n",
    "            for token_id in used_tokens:\n",
    "                logits[0, token_id] -= repetition_penalty\n",
    "\n",
    "            # probs = F.softmax(logits, dim=-1)\n",
    "\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "\n",
    "        # Step 3: top-k é‡‡æ ·\n",
    "        topk_probs, topk_indices = torch.topk(probs, k)\n",
    "        topk_probs = topk_probs.squeeze()\n",
    "        topk_indices = topk_indices.squeeze()\n",
    "\n",
    "        print(\"Topk tokens:\", [tokenizer.index_word.get(i.item()) for i in topk_indices])\n",
    "        print(\"Topk probs:\", topk_probs.tolist())\n",
    "\n",
    "        # ä» top-k ä¸­éšæœºé€‰ä¸€ä¸ª\n",
    "        sampled_index = torch.multinomial(topk_probs, 1).item()\n",
    "        next_token = topk_indices[sampled_index]\n",
    "\n",
    "        decoder_input = torch.cat([decoder_input, next_token.view(1, 1)], dim=1)\n",
    "\n",
    "        if next_token.item() == end_token:\n",
    "            break\n",
    "\n",
    "    # è§£ç ä¸ºæ–‡æœ¬\n",
    "    output_tokens = decoder_input.squeeze().tolist()[1:]  # å»æ‰ <start>\n",
    "    decoded = [\n",
    "        tokenizer.index_word.get(tok, '<UNK>')\n",
    "        for tok in output_tokens if tok != end_token\n",
    "    ]\n",
    "    return ' '.join(decoded)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ea0381d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Tokenizer is loaded successfully: /tokenizer/tokenizerForHealthCare.pkl\n",
      "ğŸ¤– Bot: welcome welcome welcome welcome welcome welcome welcome welcome welcome welcome welcome welcome welcome welcome welcome welcome welcome welcome welcome welcome the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n"
     ]
    }
   ],
   "source": [
    "from myTokenizer import myTokenizer  \n",
    "# åŠ è½½ tokenizer & æ¨¡å‹\n",
    "ThisTokenizer = myTokenizer(num_words=10000)\n",
    "tokenizer = ThisTokenizer.load_tokenizer('/tokenizer/tokenizerForHealthCare.pkl')  # ä¿®æ”¹ä¸ºå®é™…è·¯å¾„\n",
    "vocab_size = tokenizer.num_words + 1\n",
    "\n",
    "model = TransformerChat(input_vocab_size=vocab_size, target_vocab_size=vocab_size)\n",
    "model.load_state_dict(torch.load('checkpoint/weight_transformer_3550.pth', map_location='cuda'))  # ä¿®æ”¹ä¸ºä½ ä¿å­˜çš„æ¨¡å‹è·¯å¾„\n",
    "model = model.to('cuda')  # æˆ– 'cuda'\n",
    "\n",
    "# è¿›è¡Œæ¨ç†\n",
    "# response = greedy_decode(model, tokenizer, \"I feel tired and dizzy\", device='cuda')\n",
    "response = beam_search_decode(model, tokenizer, \"MY HEAD FEEL TERRIBALE\", beam_width=5, device='cuda')\n",
    "# response = top_k_sampling_decode(\n",
    "#     model, tokenizer,\n",
    "#     input_text=\"I feel tired and dizzy\",\n",
    "#     k=10,\n",
    "#     device='cuda'  # æˆ– 'cpu'\n",
    "# )\n",
    "print(\"ğŸ¤– Bot:\", response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c6a5fb34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 41977\n",
      "Number of words: 15001\n",
      "<UNKNOWN>: 1\n",
      ".: 2\n",
      ",: 3\n",
      "i: 4\n",
      "and: 5\n",
      "the: 6\n",
      "to: 7\n",
      "a: 8\n",
      "is: 9\n",
      "of: 10\n",
      "you: 11\n",
      "in: 12\n",
      "for: 13\n",
      "it: 14\n",
      "your: 15\n",
      "my: 16\n",
      "Token ID for '<start>': 18\n"
     ]
    }
   ],
   "source": [
    "word2idx = tokenizer.word_index\n",
    "\n",
    "# show the length of the vocabulary\n",
    "print(f\"Vocabulary size: {len(word2idx)}\")\n",
    "print(f\"Number of words: {tokenizer.num_words + 1}\")\n",
    "\n",
    "idx2word = {v: k for k, v in word2idx.items()}\n",
    "\n",
    "for word, idx in word2idx.items():\n",
    "    print(f\"{word}: {idx}\")\n",
    "    if idx > 15:\n",
    "        break\n",
    "\n",
    "token_id = tokenizer.word_index.get(\"<start>\", \"<Not found>\")\n",
    "print(\"Token ID for '<start>':\", token_id)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dia",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
