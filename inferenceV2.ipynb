{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e167c0c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from models.miniTransformer import generate_square_subsequent_mask\n",
    "from models.miniTransformerV2 import TransformerChat\n",
    "import myTokenizer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7a310a9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def decode_with_topk_penalty(model, tokenizer, input_text, k=10, max_len=100, temperature=1.2, penalty=1.5, device='cpu'):\n",
    "    model.eval()\n",
    "    start_id = tokenizer.word_index['<start>']\n",
    "    end_id = tokenizer.word_index['<end>']\n",
    "\n",
    "    cleaned = myTokenizer.clean_text(input_text)\n",
    "    input_ids = tokenizer.texts_to_sequences([cleaned])\n",
    "    input_tensor = torch.tensor(input_ids).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        src_emb = model.pos_encoder(model.embedding(input_tensor))\n",
    "        memory = model.transformer.encoder(model.norm(src_emb))\n",
    "\n",
    "    decoder_input = torch.tensor([[start_id]], device=device)\n",
    "\n",
    "    for _ in range(max_len):\n",
    "        tgt_emb = model.pos_encoder(model.embedding(decoder_input))\n",
    "        tgt_mask = generate_square_subsequent_mask(decoder_input.size(1)).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output = model.transformer.decoder(tgt_emb, memory, tgt_mask=tgt_mask)\n",
    "            logits = model.fc_out(model.norm(output[:, -1, :])) / temperature\n",
    "\n",
    "        # Repetition penalty\n",
    "        for tok in set(decoder_input[0].tolist()):\n",
    "            logits[0, tok] -= penalty\n",
    "\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        topk_probs, topk_ids = torch.topk(probs, k)\n",
    "        next_token = topk_ids[0, torch.multinomial(topk_probs[0], 1)]\n",
    "\n",
    "        decoder_input = torch.cat([decoder_input, next_token.view(1, 1)], dim=1)\n",
    "        if next_token.item() == end_id:\n",
    "            break\n",
    "\n",
    "    output_tokens = decoder_input.squeeze().tolist()[1:]\n",
    "    return ' '.join([tokenizer.index_word.get(i, '<UNK>') for i in output_tokens if i != end_id])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ea0381d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Tokenizer is loaded successfully: /tokenizer/tokenizerForHealthCare.pkl\n",
      "ü§ñ Bot: hello and you to his life could some like <UNKNOWN> with medicine . by a effect in cases , acute episode in . you but if you with tablets 120 14 . if then you to medications . if then you with hope you to medications in pulmonary cancer so about pain makes in cases with cardiac investigation n\n",
      "ü§ñ Bot: from description contains combination a effect but you on breast effects in its ligament impingement in . 7 7 7 7 7 7 14 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 1 <UNKNOWN> like heavy exercises will nutrients and like heavy training contains vitamin a vaginal capsules a <UNKNOWN> 7 7 7 7 7 7 7 7 7 7 7 7\n"
     ]
    }
   ],
   "source": [
    "from myTokenizer import myTokenizer  \n",
    "# Âä†ËΩΩ tokenizer & Ê®°Âûã\n",
    "ThisTokenizer = myTokenizer(num_words=10000)\n",
    "tokenizer = ThisTokenizer.load_tokenizer('/tokenizer/tokenizerForHealthCare.pkl')  # ‰øÆÊîπ‰∏∫ÂÆûÈôÖË∑ØÂæÑ\n",
    "vocab_size = tokenizer.num_words + 1\n",
    "\n",
    "model = TransformerChat(vocab_size=vocab_size)\n",
    "model.load_state_dict(torch.load('checkpoint/weight_transformerV2_3550.pth', map_location='cuda'))  # ‰øÆÊîπ‰∏∫‰Ω†‰øùÂ≠òÁöÑÊ®°ÂûãË∑ØÂæÑ\n",
    "model = model.to('cuda')  # Êàñ 'cuda'\n",
    "\n",
    "response = decode_with_topk_penalty(model, tokenizer, \"I feel tired and dizzy\", device='cuda')\n",
    "print(\"ü§ñ Bot:\", response)\n",
    "\n",
    "test2 = \"\"\"My doctor had issue with finding the baby during my transvaginal ultrasound is that because I had to pee she could see the yoke sac and I measured 9 weeks but what she thought was the baby was measuring 7 weeks. I am just worried and want to know if that interfered\"\"\"\n",
    "\n",
    "response = decode_with_topk_penalty(model, tokenizer, test2, device='cuda')\n",
    "print(\"ü§ñ Bot:\", response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c6a5fb34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 41977\n",
      "Number of words: 15001\n",
      "<UNKNOWN>: 1\n",
      ".: 2\n",
      ",: 3\n",
      "i: 4\n",
      "and: 5\n",
      "the: 6\n",
      "to: 7\n",
      "a: 8\n",
      "is: 9\n",
      "of: 10\n",
      "you: 11\n",
      "in: 12\n",
      "for: 13\n",
      "it: 14\n",
      "your: 15\n",
      "my: 16\n",
      "Token ID for '<start>': 18\n"
     ]
    }
   ],
   "source": [
    "word2idx = tokenizer.word_index\n",
    "\n",
    "# show the length of the vocabulary\n",
    "print(f\"Vocabulary size: {len(word2idx)}\")\n",
    "print(f\"Number of words: {tokenizer.num_words + 1}\")\n",
    "\n",
    "idx2word = {v: k for k, v in word2idx.items()}\n",
    "\n",
    "for word, idx in word2idx.items():\n",
    "    print(f\"{word}: {idx}\")\n",
    "    if idx > 15:\n",
    "        break\n",
    "\n",
    "token_id = tokenizer.word_index.get(\"<start>\", \"<Not found>\")\n",
    "print(\"Token ID for '<start>':\", token_id)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dia",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
